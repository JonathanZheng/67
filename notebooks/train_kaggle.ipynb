{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Six Seven Gesture Recognition Training\n",
    "\n",
    "Two-head model: Detection (binary) + Counting (multi-class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q pytorchvideo av"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import av\n",
    "import random\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFIG = {\n",
    "    'frames_per_clip': 48,\n",
    "    'spatial_size': 224,\n",
    "    'batch_size': 4,\n",
    "    'learning_rate': 3e-4,\n",
    "    'weight_decay': 0.01,\n",
    "    'epochs': 20,\n",
    "    'early_stopping_patience': 5,\n",
    "    'dropout': 0.3,\n",
    "    'num_count_classes': 12\n",
    "}\n",
    "\n",
    "DATA_DIR = Path('/kaggle/input/sixseven-gesture')\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Device: {device}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VideoTransform:\n",
    "    def __init__(self, spatial_size=224, training=True):\n",
    "        self.spatial_size = spatial_size\n",
    "        self.training = training\n",
    "        self.mean = torch.tensor([0.485, 0.456, 0.406]).view(3, 1, 1, 1)\n",
    "        self.std = torch.tensor([0.229, 0.224, 0.225]).view(3, 1, 1, 1)\n",
    "\n",
    "    def __call__(self, frames):\n",
    "        c, t, h, w = frames.shape\n",
    "        scale = self.spatial_size / min(h, w)\n",
    "        new_h, new_w = int(h * scale), int(w * scale)\n",
    "        frames = frames.view(c * t, 1, h, w)\n",
    "        frames = F.interpolate(frames, size=(new_h, new_w), mode='bilinear', align_corners=False)\n",
    "        frames = frames.view(c, t, new_h, new_w)\n",
    "\n",
    "        if self.training:\n",
    "            if new_h > self.spatial_size and new_w > self.spatial_size:\n",
    "                top = random.randint(0, new_h - self.spatial_size)\n",
    "                left = random.randint(0, new_w - self.spatial_size)\n",
    "                frames = frames[:, :, top:top+self.spatial_size, left:left+self.spatial_size]\n",
    "            if random.random() < 0.5:\n",
    "                frames = frames.flip(dims=[3])\n",
    "            brightness = random.uniform(0.8, 1.2)\n",
    "            frames = (frames * brightness).clamp(0, 1)\n",
    "        else:\n",
    "            top = (new_h - self.spatial_size) // 2\n",
    "            left = (new_w - self.spatial_size) // 2\n",
    "            frames = frames[:, :, top:top+self.spatial_size, left:left+self.spatial_size]\n",
    "\n",
    "        if frames.shape[2] != self.spatial_size or frames.shape[3] != self.spatial_size:\n",
    "            frames = F.interpolate(\n",
    "                frames.view(c * t, 1, frames.shape[2], frames.shape[3]),\n",
    "                size=(self.spatial_size, self.spatial_size),\n",
    "                mode='bilinear', align_corners=False\n",
    "            ).view(c, t, self.spatial_size, self.spatial_size)\n",
    "\n",
    "        frames = (frames - self.mean) / self.std\n",
    "        return frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GestureDataset(Dataset):\n",
    "    def __init__(self, annotations_csv, root_dir, num_frames=48, transform=None, split=None):\n",
    "        self.root_dir = Path(root_dir)\n",
    "        self.num_frames = num_frames\n",
    "        self.transform = transform\n",
    "        df = pd.read_csv(annotations_csv)\n",
    "\n",
    "        if split:\n",
    "            np.random.seed(42)\n",
    "            indices = np.random.permutation(len(df))\n",
    "            n = len(df)\n",
    "            if split == 'train':\n",
    "                df = df.iloc[indices[:int(0.7 * n)]]\n",
    "            elif split == 'val':\n",
    "                df = df.iloc[indices[int(0.7 * n):int(0.85 * n)]]\n",
    "            elif split == 'test':\n",
    "                df = df.iloc[indices[int(0.85 * n):]]\n",
    "\n",
    "        self.annotations = df.reset_index(drop=True)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.annotations)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.annotations.iloc[idx]\n",
    "        video_path = self.root_dir / row['video_path']\n",
    "        \n",
    "        is_gesture = torch.tensor(row['is_gesture'], dtype=torch.float32)\n",
    "        cycle_count = torch.tensor(min(row['cycle_count'], 11), dtype=torch.long)\n",
    "\n",
    "        container = av.open(str(video_path))\n",
    "        frames_list = list(container.decode(video=0))\n",
    "        total_frames = len(frames_list)\n",
    "\n",
    "        if total_frames <= self.num_frames:\n",
    "            indices = np.arange(total_frames)\n",
    "        else:\n",
    "            indices = np.linspace(0, total_frames - 1, self.num_frames, dtype=int)\n",
    "\n",
    "        selected = [frames_list[i].to_ndarray(format='rgb24') for i in indices if i < len(frames_list)]\n",
    "        while len(selected) < self.num_frames:\n",
    "            selected.append(selected[-1] if selected else np.zeros((224, 224, 3), dtype=np.uint8))\n",
    "\n",
    "        container.close()\n",
    "        frames = torch.from_numpy(np.stack(selected)).permute(3, 0, 1, 2).float() / 255.0\n",
    "\n",
    "        if self.transform:\n",
    "            frames = self.transform(frames)\n",
    "\n",
    "        return frames, is_gesture, cycle_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TwoHeadGestureModel(nn.Module):\n",
    "    def __init__(self, num_count_classes=12, pretrained=True, dropout=0.3):\n",
    "        super().__init__()\n",
    "        self.backbone = torch.hub.load('facebookresearch/pytorchvideo', 'slow_r50', pretrained=pretrained)\n",
    "        \n",
    "        in_features = self.backbone.blocks[-1].proj.in_features\n",
    "        self.backbone.blocks[-1].proj = nn.Identity()\n",
    "        \n",
    "        self.detection_head = nn.Sequential(\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(in_features, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "        self.counting_head = nn.Sequential(\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(in_features, num_count_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        features = self.backbone(x)\n",
    "        if features.dim() > 2:\n",
    "            features = features.mean(dim=[2, 3, 4])\n",
    "        detection = self.detection_head(features).squeeze(-1)\n",
    "        count_logits = self.counting_head(features)\n",
    "        return detection, count_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transform = VideoTransform(spatial_size=CONFIG['spatial_size'], training=True)\n",
    "val_transform = VideoTransform(spatial_size=CONFIG['spatial_size'], training=False)\n",
    "\n",
    "train_dataset = GestureDataset(\n",
    "    annotations_csv=str(DATA_DIR / 'annotations.csv'),\n",
    "    root_dir=str(DATA_DIR),\n",
    "    num_frames=CONFIG['frames_per_clip'],\n",
    "    transform=train_transform,\n",
    "    split='train'\n",
    ")\n",
    "\n",
    "val_dataset = GestureDataset(\n",
    "    annotations_csv=str(DATA_DIR / 'annotations.csv'),\n",
    "    root_dir=str(DATA_DIR),\n",
    "    num_frames=CONFIG['frames_per_clip'],\n",
    "    transform=val_transform,\n",
    "    split='val'\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=CONFIG['batch_size'], shuffle=True, num_workers=2)\n",
    "val_loader = DataLoader(val_dataset, batch_size=CONFIG['batch_size'], shuffle=False, num_workers=2)\n",
    "\n",
    "print(f'Train: {len(train_dataset)}, Val: {len(val_dataset)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TwoHeadGestureModel(\n",
    "    num_count_classes=CONFIG['num_count_classes'],\n",
    "    pretrained=True,\n",
    "    dropout=CONFIG['dropout']\n",
    ").to(device)\n",
    "\n",
    "bce_loss = nn.BCELoss()\n",
    "ce_loss = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=CONFIG['learning_rate'], weight_decay=CONFIG['weight_decay'])\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=CONFIG['epochs'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, loader):\n",
    "    model.train()\n",
    "    total_loss, det_correct, count_correct, count_total, total = 0, 0, 0, 0, 0\n",
    "    \n",
    "    for frames, is_gesture, cycle_count in tqdm(loader, desc='Train'):\n",
    "        frames, is_gesture, cycle_count = frames.to(device), is_gesture.to(device), cycle_count.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        detection, count_logits = model(frames)\n",
    "        \n",
    "        detection_loss = bce_loss(detection, is_gesture)\n",
    "        \n",
    "        positive_mask = is_gesture == 1\n",
    "        if positive_mask.any():\n",
    "            counting_loss = ce_loss(count_logits[positive_mask], cycle_count[positive_mask])\n",
    "        else:\n",
    "            counting_loss = torch.tensor(0.0, device=device)\n",
    "        \n",
    "        loss = detection_loss + counting_loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        det_correct += ((detection > 0.5).float() == is_gesture).sum().item()\n",
    "        total += is_gesture.size(0)\n",
    "        \n",
    "        if positive_mask.any():\n",
    "            count_preds = count_logits[positive_mask].argmax(dim=-1)\n",
    "            count_correct += (torch.abs(count_preds - cycle_count[positive_mask]) <= 1).sum().item()\n",
    "            count_total += positive_mask.sum().item()\n",
    "    \n",
    "    return total_loss / len(loader), det_correct / total, count_correct / count_total if count_total > 0 else 0\n",
    "\n",
    "def validate(model, loader):\n",
    "    model.eval()\n",
    "    total_loss, det_correct, count_correct, count_total, total = 0, 0, 0, 0, 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for frames, is_gesture, cycle_count in tqdm(loader, desc='Val'):\n",
    "            frames, is_gesture, cycle_count = frames.to(device), is_gesture.to(device), cycle_count.to(device)\n",
    "            \n",
    "            detection, count_logits = model(frames)\n",
    "            \n",
    "            detection_loss = bce_loss(detection, is_gesture)\n",
    "            \n",
    "            positive_mask = is_gesture == 1\n",
    "            if positive_mask.any():\n",
    "                counting_loss = ce_loss(count_logits[positive_mask], cycle_count[positive_mask])\n",
    "            else:\n",
    "                counting_loss = torch.tensor(0.0, device=device)\n",
    "            \n",
    "            loss = detection_loss + counting_loss\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            det_correct += ((detection > 0.5).float() == is_gesture).sum().item()\n",
    "            total += is_gesture.size(0)\n",
    "            \n",
    "            if positive_mask.any():\n",
    "                count_preds = count_logits[positive_mask].argmax(dim=-1)\n",
    "                count_correct += (torch.abs(count_preds - cycle_count[positive_mask]) <= 1).sum().item()\n",
    "                count_total += positive_mask.sum().item()\n",
    "    \n",
    "    return total_loss / len(loader), det_correct / total, count_correct / count_total if count_total > 0 else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_val_loss = float('inf')\n",
    "patience_counter = 0\n",
    "\n",
    "for epoch in range(CONFIG['epochs']):\n",
    "    print(f'\\nEpoch {epoch + 1}/{CONFIG[\"epochs\"]}')\n",
    "    train_loss, train_det_acc, train_count_acc = train_epoch(model, train_loader)\n",
    "    val_loss, val_det_acc, val_count_acc = validate(model, val_loader)\n",
    "    scheduler.step()\n",
    "\n",
    "    print(f'Train Loss: {train_loss:.4f}, Det Acc: {train_det_acc:.4f}, Count ±1: {train_count_acc:.4f}')\n",
    "    print(f'Val Loss: {val_loss:.4f}, Det Acc: {val_det_acc:.4f}, Count ±1: {val_count_acc:.4f}')\n",
    "\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        patience_counter = 0\n",
    "        torch.save(model.state_dict(), '/kaggle/working/best_model.pth')\n",
    "        print('Saved best model')\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        if patience_counter >= CONFIG['early_stopping_patience']:\n",
    "            print('Early stopping')\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load('/kaggle/working/best_model.pth'))\n",
    "model.eval()\n",
    "\n",
    "example_input = torch.randn(1, 3, CONFIG['frames_per_clip'], CONFIG['spatial_size'], CONFIG['spatial_size']).to(device)\n",
    "traced = torch.jit.trace(model, example_input)\n",
    "traced.save('/kaggle/working/model_traced.pt')\n",
    "print('Saved traced model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = GestureDataset(\n",
    "    annotations_csv=str(DATA_DIR / 'annotations.csv'),\n",
    "    root_dir=str(DATA_DIR),\n",
    "    num_frames=CONFIG['frames_per_clip'],\n",
    "    transform=val_transform,\n",
    "    split='test'\n",
    ")\n",
    "test_loader = DataLoader(test_dataset, batch_size=CONFIG['batch_size'], shuffle=False, num_workers=2)\n",
    "\n",
    "det_preds, det_labels = [], []\n",
    "count_preds, count_labels = [], []\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for frames, is_gesture, cycle_count in test_loader:\n",
    "        frames = frames.to(device)\n",
    "        detection, count_logits = model(frames)\n",
    "        \n",
    "        det_preds.extend((detection > 0.5).float().cpu().numpy())\n",
    "        det_labels.extend(is_gesture.numpy())\n",
    "        \n",
    "        positive_mask = is_gesture == 1\n",
    "        if positive_mask.any():\n",
    "            count_preds.extend(count_logits[positive_mask].argmax(dim=-1).cpu().numpy())\n",
    "            count_labels.extend(cycle_count[positive_mask].numpy())\n",
    "\n",
    "det_preds, det_labels = np.array(det_preds), np.array(det_labels)\n",
    "count_preds, count_labels = np.array(count_preds), np.array(count_labels)\n",
    "\n",
    "det_acc = (det_preds == det_labels).mean()\n",
    "fpr = det_preds[det_labels == 0].sum() / (det_labels == 0).sum() if (det_labels == 0).sum() > 0 else 0\n",
    "\n",
    "count_acc = (np.abs(count_preds - count_labels) <= 1).mean() if len(count_preds) > 0 else 0\n",
    "mae = np.abs(count_preds - count_labels).mean() if len(count_preds) > 0 else 0\n",
    "\n",
    "print(f'Detection Accuracy: {det_acc:.4f}')\n",
    "print(f'False Positive Rate: {fpr:.4f}')\n",
    "print(f'Counting ±1 Accuracy: {count_acc:.4f}')\n",
    "print(f'Counting MAE: {mae:.4f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
